\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.99\linewidth}
    \begin{subfigure}[t]{0.32\linewidth}
      \def\svgwidth{0.99\columnwidth}
      \input{./figs/adv_clean_ortho_dim_auc_imgspace.pdf_tex}
      \subcaption{Input Space}
      \label{fig:dir-adv-att-inp-01}
    \end{subfigure}
    % \begin{subfigure}[t]{0.32\linewidth}
    %   \def\svgwidth{0.99\columnwidth}
    %   \input{./figs/adv_clean_ortho_dim_auc.pdf_tex}
    %   \subcaption{TODO Layer 1}
    %   \label{fig:dir-adv-att-layer0-01}
    % \end{subfigure}
  \begin{subfigure}[t]{0.32\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/adv_clean_ortho_dim_auc.pdf_tex}
    \subcaption{Layer 2}
    \label{fig:dir-adv-att-layer1-01}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/adv_clean_ortho_dim_auc_layer2_01.pdf_tex
    }
    \subcaption{Layer 3}
    \label{fig:dir-adv-att-layer2-01}
  \end{subfigure}
  \subcaption*{Perturbation Budget~\(\epsilon=0.1\)}
\end{subfigure}
\begin{subfigure}[t]{0.99\linewidth}
  \begin{subfigure}[t]{0.32\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/adv_clean_ortho_dim_auc_imgspace_03.pdf_tex}
    \subcaption{Input Space}
    \label{fig:dir-adv-att-inp-03}
  \end{subfigure}
  % \begin{subfigure}[t]{0.32\linewidth}
  %     \def\svgwidth{0.99\columnwidth}
  %     \input{./figs/adv_clean_ortho_dim_auc.pdf_tex}
  %     \subcaption{TODO Layer 1}
  %     \label{fig:dir-adv-att-layer0-03}
  %   \end{subfigure}
  \begin{subfigure}[t]{0.32\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/adv_clean_ortho_dim_auc_03.pdf_tex}
    \subcaption{Layer 2}
    \label{fig:dir-adv-att-layer1-03}
  \end{subfigure}
  \begin{subfigure}[t]{0.32\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/adv_clean_ortho_dim_auc_layer2_03.pdf_tex}
    \subcaption{Layer 3}
    \label{fig:dir-adv-att-layer2-03}
  \end{subfigure}
  \subcaption*{Perturbation Budget~\(\epsilon=0.3\)}
\end{subfigure}
\caption{Increasing the Adv subspace dimensions and the data dimension increases the classifiability of adversarial example. The vertical lines are isolines for the sum of the two adversarial subspace dimension and the data subspace dimension.}
\label{fig:auroc-detector-mnist}
\end{figure}


We verify our strategy on the MNIST dataset with a standard 4-layered CNN. We
extract representations from the input space~\(\ell=0\)~(784 dimensional), after
the second layer~\(\ell=2\)~(9260 dimensional), and before the last linear
layer~\(\ell=3\)~(128 dimensional). Then for varying values of the data subspace
dimension~\(s_1\) and adversarial subspace dimension~\(s_2\), we compute the energy ratio score function~(\Cref{eq:energy-ratio-score-fn}) and then compute the AUROC score of the adversary detector function~(\Cref{eq:detector-fn}) \(\psi_{s_1, s_2, \rho} \)  by varying \(\rho\). We also vary the strength of the adversary by experimenting with two perturbation budgets \(\epsilon\in\bc{0.1, 0.3}\) for the \(\ell_\infty\) PGD adversary.

The results are plotted in~\Cref{fig:auroc-detector-mnist}. Using either the
input layer or layer \(2\) gives us a high AUROC score, compared to the \(3^{\it
rd}\) layer. However, the input layer is more sensitive to the hyper-parameters
\(s_1\) and \(s_2\)~(the X and the Y axis respectively), whereas layer \(2\) is
less sensitive to hyperparameters.

This is possibly because the 2nd layer is high-dimensional and thus has a larger Null space compared to the input space. To avoid, hyperpameter tuning, one might prefer to use the representation space compared to the input space. It is also interesting that the later layers perform poorly, mostly because at that point the representations have become more label specific and look very mucg alike for both clean and adversarial points. The fact that that layer has a dimension of only \(128\) makes it even harder to separate the two.

\paragraph{Things to think about}
\begin{enumerate}
  \item Construct Adaptive Attacks to show limitations.
  \item Show results on CIFAR10, CIFAR100 with larger datasets
  \item Show theoretically that for a 1/2 layered MLP trained using GD on a low rank dataset, the adversarial examples have significant weight on the null space.
\end{enumerate}
% AUC values for represeantation
% ([500, 800, 1000, 2000, 5000],
%  [480, 600, 900, 1500, 3000],
%  array([[0.86347451, 0.871213  , 0.88653578, 0.90413287, 0.92836279],
%         [0.89912602, 0.90562672, 0.91786264, 0.93481801, 0.95354976],
%         [0.91893012, 0.92618109, 0.93657281, 0.94936903, 0.96454761],
%         [0.96882571, 0.972665  , 0.97610959, 0.98073429, 0.98755096],
%         [0.9955351 , 0.99602749, 0.9963835 , 0.99699675, 0.9979373 ]]))

% Input Space, epsilon = 0.1
% ([10, 40, 60, 100, 150, 200, 300, 400, 500, 600, 700],
%                 [10, 40, 60, 100, 150, 200, 300, 400, 500, 600, 700],
% array([0.51040031, 0.52102429, 0.52924558, 0.54309783, 0.55564324,
%        0.56431016, 0.57652512, 0.58696801, 0.59886409, 0.6107899 ,
%        0.62326317, 0.54148327, 0.58219431, 0.60046205, 0.63070298,
%        0.65760973, 0.67688118, 0.70443289, 0.72971774, 0.75741137,
%        0.78376916, 0.64425812, 0.57933831, 0.63355185, 0.65749986,
%        0.69258209, 0.72400582, 0.74749131, 0.77857949, 0.80922246,
%        0.83990726, 0.86721279, 0.64080698, 0.65152707, 0.73285938,
%        0.75790812, 0.79398144, 0.82473629, 0.84698895, 0.87958281,
%        0.90964099, 0.93380352, 0.95396441, 0.62212813, 0.70005103,
%        0.7977662 , 0.8262664 , 0.86241656, 0.8923909 , 0.91015163,
%        0.93974063, 0.96080231, 0.97598722, 0.58126403, 0.58691334,
%        0.71802282, 0.83889524, 0.87542676, 0.90875713, 0.93018094,
%        0.9447849 , 0.96843691, 0.98194501, 0.99027767, 0.58544015,
%        0.57094244, 0.7937668 , 0.90980673, 0.93351598, 0.956924  ,
%        0.97147768, 0.97859921, 0.98856016, 0.99469281, 0.53413303,
%        0.54578276, 0.55039775, 0.92007545, 0.96746508, 0.97454148,
%        0.98137734, 0.98626916, 0.99071665, 0.99621426, 0.53520483,
%        0.5225559 , 0.52736544, 0.53440267, 0.98058992, 0.98864524,
%        0.99059757, 0.99332973, 0.99582868, 0.99769376, 0.493835  ,
%        0.49260382, 0.49632017, 0.50485975, 0.51585451, 0.99658275,
%        0.99837821, 0.99856373, 0.99917121, 0.46279653, 0.47070894,
%        0.4683721 , 0.47036328, 0.4751615 , 0.48660017, 0.49822666,
%        1.        , 0.43784737, 0.43260276, 0.43249963, 0.43321566,
%        0.43774866, 0.44016985, 0.44696497, 0.4563543 , 0.46767703,
%        0.47932477])
Consider a $k$-class classification problem defined on a data distribution
\(\cD\) over \(\reals^d\times\cY\). The goal of learning is to produce a set of
parameters \(\theta\) such that a neural network \(f_{\theta}\), parameterised
by \(\theta\), obtains a small expected error~\(\risk{\cD}{f}\).

\begin{restatable}[Expected risk]{defn}{emprisk}
    \label{defn:exp_loss}
    For a distribution $\cD$, and a neural network $f_\theta$, the expected risk
    is defined as 
  \[  \risk{\cD}{f}=\bP_{\br{\vec{x},y}\sim\cD}\bs{f\br{\vec{x}}\neq y},\]
  \end{restatable}

However, there are two main challenges to his. First, despite being fixed, the
distribution \(\cD\) is unknown to the learner and hence, the learner cannot
directly optimise the expected risk. Instead, the learner is presented with an
$N$-sized training dataset \(S_N \sim \cD^N\) sampled i.i.d. from \(\cD\). A neural
network \(f_\theta\) is then trained to minimise the empirical classification
error \(\empRisk{h}{N}\) on \(S_N\) with the hope that a small empirical
classification error~\(\empRisk{h}{N}\) would also translate to low expected
risk \(\risk{\cD}{f}\). 
\begin{defn}[Empirical risk]
    \label{defn:emp_loss}
    If the training set consists of $\{(\vec{x}_i, y_i)\}$ for $i \in \{1\cdots
    N\}$ then the empirical loss is defined as
    \[\empRisk{h}{N} = \frac{1}{N} \sum_{i=1}^N \bI\bs{h\br{\vec{x}_i} \neq y_i}\]
  \end{defn}

  Second, it is also hard to estimate the expected error~\(\risk{\cD}{f}\) as
  the learner does not have access to the distribution \(\cD\). Instead the
  learner has access to a test dataset, another set of i.i.d. samples from
  \(\cD\). To test the performance of a neural network, its error on this test set is considered as a proxy for~\(\risk{\cD}{f}\).
   
  For many modern over-parameterised neural network, experiments have usually
  borne out that a low empirical error on the train set also leads to a low test
  error. However, recent
  works~\citep{goodfellow2014explaining,szegedy2013intriguing} have shown that
  despite achieving low test error these neural networks are susceptible to
  \(\ell_p\) adversarial attacks and consequently, has a very high \(\ell_p\)
  adversarial error.

\begin{restatable}[$\ell_p$ Adversarial
  Error]{defn}{advrisk}\label{defn:adv_risk} For any distribution $\cD$
  defined over $\br{\vec{x},y}\in\reals^d\times\cY$, any classifier
  $h:\reals^d\rightarrow\cY$, and any $\gamma>0$, 
  %%
  the $\gamma$-\emph{adversarial} error is 
          \begin{equation} 
              \radv{\gamma}{h;\cD}=\bP_{\br{\vec{x},y} 
              \sim\cD}\bs{\exists \vec{z}\in\cB_{\gamma}
              \br{\vec{x}};h\br{\vec{z}}\neq y},
          \end{equation} 
  where $\cB_\gamma^p\br{\vec{x}}$ is the $\ell_p$ ball of radius $\gamma \ge
  0$ around $\vec{x}$ under the $\ell_p$ norm.
\end{restatable}

The observation that these state-of-the-art neural networks have a low test
error but a high adversarial error indicate that while they are well-behaved on
the manifold of the observed data~(low test error), their behavior is less
predictable if the data is moved slightly away from the manifold~(high
adversarial error). An algorithm that can detect whether there is a large mass away from the manifold can possibly detect adversarial examples. In this paper, we make this intuition more rigorous and present an algorithm to do the same.

\begin{figure}[t]
  \centering
  \def\svgwidth{0.99\columnwidth}
  \input{./figs/adv_2d_subspace_noiseless.pdf_tex}
  \caption{The left plot shows the distribution of the training data ~(two
  classes represented by {\color{orange} orange} and {\color{green} green}
  circles). The right figure shows the construction of adversarial attacks on
  the same classifier using a test set. The {\color{red} red} and
  {\color{green} green} crosses~(\(\times \)) are the adversarially vulnerable
  test points, the {\color{blue} blue arrows} are the direction of adversarial
  perturbation, and the {\color{red} red} and {\color{green} green}
  circles~(\(\circ\)) are the adversarially perturbed points.}
  \label{fig:dir-adv-att}
\end{figure}

\subsection{Toy Experiment with non-isotropic multivariate gaussian}
To gain an intuition, we create a simple 2-dimensional binary classification
task in~\Cref{fig:dir-adv-att}. The data is sampled from a mixture of two
two-dimensional non-isotropic gaussians centered at \(\br{-2,0}\) and
\(\br{2,0}\) respectively. The covariance matrices of both gaussians are
diagonal with diagonal entries \(1\) and  \(0.05\). To sample a point from the
distribution, first a label is sampled \(y\sim\mathrm{Unif}\br{\bc{-1,1}}\) and
then the covariates are sampled from the above mentioned gaussian centered at
\(2y\). Note that, out of the two coordinates the main variance in the
covariates is along the first coordinate, denoted as the {\emph principal
component} in~\Cref{fig:dir-adv-att}, with a small variance along the second
coordinate, denoted as the non-principal component in~\Cref{fig:dir-adv-att}.

Interestingly, the arrows in~\Cref{fig:dir-adv-att} show that all adversarial
attacks have a significant component along the non-principal component i.e.
along the vertical direction, even when adversarial attacks are possible by only
moving horizontally along the principal component. Thus, the adversarially
perturbed points~({\color{red} red} and {\color{green} green} circles) tend to
have a larger~(in magnitude) vertical component than the clean test
points~({\color{orange} orange} and {\color{green} green} circles). 

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.99\linewidth}
  \begin{subfigure}[t]{0.16\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/diff_comp_2mog_6dim_1prin.pdf_tex}
    \subcaption*{1 PC}
    \label{fig:1-prin-comp}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/diff_comp_2mog_6dim_2prin.pdf_tex}
    \subcaption*{2 PC}
    \label{fig:2-prin-comp}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/diff_comp_2mog_6dim_3prin.pdf_tex}
    \subcaption*{3 PC}
    \label{fig:3-prin-comp}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/diff_comp_2mog_6dim_4prin.pdf_tex}
    \subcaption*{4 PC}
    \label{fig:4-prin-comp}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/diff_comp_2mog_6dim_5prin.pdf_tex}
    \subcaption*{5 PC}
    \label{fig:5-prin-comp}
  \end{subfigure}
  \begin{subfigure}[t]{0.16\linewidth}
    \def\svgwidth{0.99\columnwidth}
    \input{./figs/diff_comp_2mog_6dim_6prin.pdf_tex}
    \subcaption*{6 PC}
    \label{fig:6-prin-comp}
  \end{subfigure}
  % \subcaption{Energy on Various Components for clean and adversarial examples.}
\end{subfigure}
  % \begin{subfigure}[c]{0.3\linewidth}\centering
  %   % \begin{subfigure}[t]{0.99\linewidth}
  %   %   \def\svgwidth{0.99\columnwidth}
  %   %   \input{./figs/128_dim_random_acc_null_space.pdf_tex}
  %   %   \subcaption{}
  %   %   \label{fig:128-dim-accs}
  %   % \end{subfigure}
  %   \begin{subfigure}[t]{0.99\linewidth}
  %     \def\svgwidth{0.99\columnwidth}
  %     \input{./figs/128_dim_random_adv_dim_null_space.pdf_tex}
  %     \subcaption{}
  %     \label{fig:128-dim-null-space-adv-space}
  %   \end{subfigure}
    % \caption{Sufficient dimension of Adv Subspace}
% \end{subfigure}
  \caption{Adversarial example puts energy on non Principal Components~(PC).
  In~\Cref{fig:6-dim-gauss}, X axis indicates the index of the component and Y
  axis represents the average energy of the {\color{blue} clean} and
  {\color{orange} adversarial} examples on that component. The caption of each
  figure indicates the number of principal components in the data.
  }
  \label{fig:6-dim-gauss}
\end{figure}

Next, we extend this to \(6\)-dimensional gaussians. We vary the number of
principal and non-principal components in the data and then measure the energy
of the clean and the adversarial data points along these principal and
non-principal components. To set the number of principal and non-principal
components to \(s\) and \(6-s\), we simply change the diagonal entries of
covariance matrix to \(s\) ones followed by \(6-s\) times \(0.05\)s
respectively. We train a a neural network on a dataset sampled from this
distribution just like the two-dimensional example above and create adversarial
examples using PGD. Then, we compute the principal and non-principal components
of the training data using PCA and obtain the projection of each clean and
adversarial example on each these PCA components. This experiment is repeated
for all \(s\in\bc{1,\cdots,6}\).

In~\Cref{fig:6-dim-gauss}, we plot the average energy of the clean data and the
adversarial data on each of the PCA components for varying number of principal
and non-principal components. For example, the experiment in  the first figure
of~\Cref{fig:6-dim-gauss} has \(1\) principal component and \(5\) non-principal
components whereas the experiment in the last figure has \(6\) principal
components and \(0\) non-principal components. The {\color{orange} orange} lines
show the average energy of adversarial examples on each component and the
{\color{blue} blue} line shows the average energy of the clean example on each
component.

\begin{wrapfigure}{r}{0.5\linewidth}\vspace{-10pt}
  \def\svgwidth{0.99\linewidth}
  \input{./figs/128_dim_random_adv_dim_null_space.pdf_tex}
  \caption{The X-axis plots the dimension of the null space~(128 - rank) of the
  distribution. The Y-axis plots the number of dimensions where the adversarial
  examples have at least \(10^5\) times the energy of the clean examples.}
  \label{fig:128-dim-null-space-adv-space}\vspace{-15pt}
\end{wrapfigure}

In~\Cref{fig:128-dim-null-space-adv-space}, we can see this more explicitly ---
that the number of dimensions where the adversarial examples have significantly
more energy than the clean examples is equal to the dimension of the null-space
of the data. The set of learning tasks is a set of 128-dimensional binary
classification tasks where each of them vary by the rank of the data
distribution. Out of the \(128\) dimensions, \(10\) dimensions are correlated
with the label, \(\mathrm{dim}\br{\mathrm{Null Space}}\) features are random
linear combinations of these\(10\) features and the rest of the features are
random normally distributed. Thus, with a very probability the rank of the
distribution is \(128 - \mathrm{dim}\br{\mathrm{Null Space}}\). 

For each of the distributions, we sample \(5000\) training points, learn an MLP
on that set, and then create adversarial examples. Then, we project the clean
and adversarial examples on the \(128\) principal components and measure the
energy on each component. The Y-axis plots the number of such components where
the adversarial examples have at least \(10^5\) times the energy of the clean
examples. Thus, the less the null dimension, the harder it is to distinguish the
clean from the adversarial examples. 

~\Cref{fig:6-dim-gauss,fig:128-dim-null-space-adv-space} clearly shows that the
adversarial examples have a similar energy on the principal components as clean
examples but have a much higher energy on the non-principal components than the
clean examples. We use this as a motivation to define the Energy Ratio Score
function in the next section.


\subsection{Defining the Energy Ratio Score function}
Let the data matrix be \(S_{N\vert X}\in\reals^{N\times d}\) and consider $f:
\mathbb{R}^d \mapsto \mathbb{R}^k$ to be a feed-forward MLP that maps $d$
dimensional input $\vx$ to a $k$ dimensional output $\vy$ with the width of the
\(\ell^{\it th}\) layer being \(m_\ell\). We can decompose this into two
sub-networks, one consisting of the layers before the $\ell^{\it th}$ layer and
one after i.e.  $f(\vx) = f^{+}_\ell\br{f^{-}_\ell(\vx; \phi) ; \theta}$, where
$f^{-}_\ell (.;\phi)$, parameterised by $\phi$, represents the part of the
network up to layer $\ell$ and, $f^{+}_\ell(.;\theta)$ represents the part of
the network thereafter. With this notation, the $m_\ell$ dimensional
representation (or the activations) after any layer $\ell$ can simply be written
as $\va = f^{-}_\ell(\vx; \phi) \in \mathbb{R}^{m_\ell}$ and we call the
matrix,formed by stacking all of the representations from the \(\ell^{\it th}\)
layer, the representation  matrix of the \(\ell^{\it th}\) layer
\(\vec{A}^{\ell}\in\reals^{N\times m_\ell}\). For ease of notation, we refer to
the input matrix  as the representation matrix for the \(0^{\it th}\) layer i.e.
\(S_{N\vert X}=\vec{A}^0\) and \(m_0 = d\).

For any layer's~(including the input) representation matrix, we define the {\em
data subspace} as the space spanned by the top eigenvectors of the
representation matrix and the {\em adversarial subspace} as the space spanned by
the rows of the matrix but orthogonal to the data subspace. For any layer, let
\(\bc{c_1,\cdots,c_d}\) be the corresponding principal components in descending
order of the variance of data explained by each component. For two positive
integers \(1\le s_1\le s_2\le d\), the {\em data subspace} is defined as the
space spanned by \(\{c_1,\cdots,c_{s_1}\}\) and the orthogonal space is the
space spanned by \(c_{s_1+1},\cdots,c_{s_2}\). Then for every point
\(\vec{x}\in\reals^{m_\ell}\), we assign a score 


\begin{equation}\label{eq:energy-ratio-score-fn}
  \rho_{s_1,s_2}\br{\vec{x}} = \sqrt{\dfrac{\sum_{i=s_1+1}^{s_2}\ip{\vec{c_i}}{\vec{x}}^2}{ \sum_{i=1}^{s_1} \ip{\vec{c_i}}{\vec{x}}^2}}
\end{equation}

Then for a given a threshold \(\rho_0\), we can define an adversary detector
\(\psi_{\rho,s_1,s_2}:\reals^{m_\ell}\mapsto \bc{0,1}\)
\[ \psi_{\rho,s_1,s_2}\br{\vec{x}} = \begin{cases} \label{eq:detector-fn}
  1 & \text{if}\enskip \rho_{s_1,s_2}\br{\vec{x}}  >\rho \\
  0  & \text{if}\enskip  \rho_{s_1,s_2}\br{\vec{x}}  \le \rho
\end{cases}
\]
 where classifying \(\vec{x}\) as \(1\) means classifying it as an adversarially perturbed example and \(0\) means classifying it as a clean example.


 